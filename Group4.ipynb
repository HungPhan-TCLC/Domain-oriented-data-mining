{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14200029,"sourceType":"datasetVersion","datasetId":9055939},{"sourceId":14206649,"sourceType":"datasetVersion","datasetId":9061165},{"sourceId":14131320,"sourceType":"datasetVersion","datasetId":9004342}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip uninstall -y torch","metadata":{"execution":{"iopub.status.busy":"2025-12-22T00:43:40.015215Z","iopub.execute_input":"2025-12-22T00:43:40.015441Z","iopub.status.idle":"2025-12-22T00:43:40.020308Z","shell.execute_reply.started":"2025-12-22T00:43:40.015416Z","shell.execute_reply":"2025-12-22T00:43:40.019499Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --extra-index-url https://download.pytorch.org/whl/cu126\n","metadata":{"execution":{"iopub.status.busy":"2025-12-22T00:43:40.022036Z","iopub.execute_input":"2025-12-22T00:43:40.022268Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m213.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/216.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U transformers bitsandbytes\n!pip install -q -U langchain langchain-community langchain-huggingface chromadb sentence-transformers\n!pip install -q -U accelerate","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\nfrom langchain_huggingface import HuggingFacePipeline\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import json\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\"\"\"\n\n# 1. Load dá»¯ liá»‡u\ntry:\n    with open('/kaggle/input/medicine-data/merged.json', 'r', encoding='utf-8') as f:\n        data = json.load(f)\nexcept FileNotFoundError:\n    # Dá»¯ liá»‡u máº«u minh há»a\n    data = [\n        {\"category\": \"Thuá»‘c\", \"title\": \"Panadol\", \"content\": \"Panadol chá»©a hoáº¡t cháº¥t Paracetamol. Thuá»‘c cÃ³ tÃ¡c dá»¥ng háº¡ sá»‘t, giáº£m Ä‘au nháº¹...\", \"source\": \"Vinmec\"},\n        {\"category\": \"Bá»‡nh\", \"title\": \"ViÃªm phá»•i\", \"content\": \"ViÃªm phá»•i lÃ  tÃ¬nh tráº¡ng nhiá»…m trÃ¹ng phá»•i. Triá»‡u chá»©ng gá»“m ho, sá»‘t cao...\", \"source\": \"BoYTe\"}\n    ]\n# Thá»‘ng kÃª dá»¯ liá»‡u Ä‘áº§u vÃ o\nprint(f\"\\n{'='*60}\")\nprint(f\"ğŸ“Š THá»NG KÃŠ Dá»® LIá»†U Äáº¦U VÃ€O\")\nprint(f\"{'='*60}\")\nprint(f\"ğŸ“ Tá»•ng sá»‘ báº£n ghi: {len(data)}\")\n# 2. Chuyá»ƒn Ä‘á»•i JSON thÃ nh LangChain Documents\ndocuments = []\nfor item in data:\n    page_content = item['content']\n    \n    metadata = {\n        \"source\": item.get(\"source\", \"\"),\n        \"title\": item.get(\"title\", \"\"),\n        \"category\": item.get(\"category\", \"\") \n    }\n    \n    documents.append(Document(page_content=page_content, metadata=metadata))\n\n# ==================== BÆ¯á»šC 3: CHUNKING (Cáº¢I TIáº¾N) ====================\nprint(\"âœ‚ï¸  Äang chia nhá» vÄƒn báº£n...\")\n# Cáº¢I TIáº¾N 2: Äiá»u chá»‰nh chunk_size phÃ¹ há»£p vá»›i vÄƒn báº£n y táº¿\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=400,  # Giáº£m xuá»‘ng 400 cho vÄƒn báº£n y táº¿ (thÆ°á»ng ngáº¯n gá»n)\n    chunk_overlap=80,  # Giáº£m overlap tÆ°Æ¡ng á»©ng\n    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \"],\n    length_function=len\n)\nsplits = text_splitter.split_documents(documents)\nprint(f\"âœ… ÄÃ£ táº¡o {len(splits)} chunks\")\n\n# ==================== BÆ¯á»šC 4: Táº O VECTOR DB ====================\nprint(\"\\nğŸ”¢ Äang táº¡o embeddings vÃ  vector database...\")\nembedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nembeddings = HuggingFaceEmbeddings(\n    model_name=embedding_model_name,\n    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n)\n\nvector_db = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    collection_name=\"medical_rag_v3\"\n)\nprint(\"âœ… Vector DB Ä‘Ã£ sáºµn sÃ ng!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom langchain_huggingface import HuggingFacePipeline\"\"\"\n\n# TÃªn mÃ´ hÃ¬nh trÃªn HuggingFace\nmodel_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\n# 1. Táº£i Tokenizer vÃ  Model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    device_map=\"auto\", \n    low_cpu_mem_usage=True\n)\n\n# 2. Táº¡o Pipeline sinh vÄƒn báº£n\ntext_generation_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=256,       \n    do_sample=False,           \n    repetition_penalty=1.1,   \n    return_full_text=False   \n)\n\n# 3. Bá»c vÃ o LangChain LLM\nllm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nprint(\"âœ… ÄÃ£ táº£i xong mÃ´ hÃ¬nh Qwen2.5-0.5B!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== BÆ¯á»šC 6: RERANKER (Má»šI) ====================\nprint(\"\\nğŸ¯ Äang táº£i Reranker model...\")\n\nclass CrossEncoderReranker:\n    \"\"\"\n    Reranker sá»­ dá»¥ng Cross-Encoder Ä‘á»ƒ xáº¿p háº¡ng láº¡i documents\n    \"\"\"\n    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model.to(self.device)\n        self.model.eval()\n    \n    def build_rerank_text(self, doc):\n        title = doc.metadata.get(\"title\", \"\")\n        content = doc.page_content\n\n        return f\"{title}. {content}\"\n\n    def rerank(self, query, documents, top_k=3):\n        \"\"\"\n        Xáº¿p háº¡ng láº¡i documents dá»±a trÃªn Ä‘á»™ liÃªn quan vá»›i query\n        \n        Args:\n            query: CÃ¢u há»i\n            documents: List cÃ¡c Document tá»« retriever\n            top_k: Sá»‘ lÆ°á»£ng documents giá»¯ láº¡i\n        \n        Returns:\n            List documents Ä‘Ã£ Ä‘Æ°á»£c xáº¿p háº¡ng láº¡i\n        \"\"\"\n        if not documents:\n            return []\n        \n        # Táº¡o pairs (query, doc)\n        pairs = [[query, self.build_rerank_text(doc)] for doc in documents]\n        \n        # TÃ­nh Ä‘iá»ƒm cho tá»«ng pair\n        with torch.no_grad():\n            inputs = self.tokenizer(\n                pairs, \n                padding=True, \n                truncation=True, \n                return_tensors='pt',\n                max_length=512\n            ).to(self.device)\n            \n            scores = self.model(**inputs).logits.squeeze(-1).cpu().numpy()\n        doc_score_pairs = list(zip(documents, scores))\n        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)\n        \n        # Láº¥y top_k documents\n        reranked_docs = [doc for doc, score in doc_score_pairs[:top_k]]\n        \n        # In thÃ´ng tin debug\n        print(f\"\\nğŸ” Reranking results:\")\n        for i, (doc, score) in enumerate(doc_score_pairs[:top_k], 1):\n            print(f\"  {i}. Score: {score:.4f} | Title: {doc.metadata.get('title', 'N/A')[:50]}\")\n        \n        return reranked_docs\n\n# Khá»Ÿi táº¡o reranker\nreranker = CrossEncoderReranker()\nprint(\"âœ… Reranker Ä‘Ã£ sáºµn sÃ ng!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== BÆ¯á»šC 7: Táº O RETRIEVER Vá»šI RERANKER ====================\nprint(\"\\nğŸ”§ Äang thiáº¿t láº­p retriever...\")\n\n# Cáº¢I TIáº¾N 4: TÄƒng sá»‘ documents ban Ä‘áº§u, sau Ä‘Ã³ rerank\nbase_retriever = vector_db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 10}  # Láº¥y 10 docs trÆ°á»›c, sau Ä‘Ã³ rerank xuá»‘ng 3\n)\n\ndef retrieve_and_rerank(query):\n    \"\"\"\n    HÃ m káº¿t há»£p retrieval vÃ  reranking\n    \"\"\"\n    # BÆ°á»›c 1: Retrieval (láº¥y 10 docs)\n    docs = base_retriever.invoke(query)\n    \n    # BÆ°á»›c 2: Rerank (giá»¯ láº¡i 3 docs tá»‘t nháº¥t)\n    reranked_docs = reranker.rerank(query, docs, top_k=3)\n    \n    return reranked_docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ==================== BÆ¯á»šC 8: PROMPT ENGINEERING (Cáº¢I TIáº¾N) ====================\n# Cáº¢I TIáº¾N 5: Prompt rÃµ rÃ ng, ngáº¯n gá»n hÆ¡n\nprompt_template = \"\"\"<|im_start|>system\nBáº¡n lÃ  trá»£ lÃ½ y táº¿ AI chuyÃªn nghiá»‡p. Tráº£ lá»i cÃ¢u há»i ÄÃºng/Sai dá»±a trÃªn vÄƒn báº£n y táº¿ Ä‘Æ°á»£c cung cáº¥p.\n\nQUY Táº®C:\n- Chá»‰ tráº£ lá»i \"ÄÃºng\" hoáº·c \"Sai\"\n- KhÃ´ng giáº£i thÃ­ch thÃªm\n- Náº¿u vÄƒn báº£n xÃ¡c nháº­n thÃ´ng tin â†’ \"ÄÃºng\"\n- Náº¿u vÄƒn báº£n phá»§ nháº­n hoáº·c khÃ´ng Ä‘á» cáº­p â†’ \"Sai\"\n<|im_end|>\n\n<|im_start|>user\nVÄƒn báº£n tham kháº£o:\n{context}\n\nCÃ¢u há»i: {question}\n\nTráº£ lá»i (chá»‰ ghi \"ÄÃºng\" hoáº·c \"Sai\"):\n<|im_end|>\n\n<|im_start|>assistant\n\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template, \n    input_variables=[\"context\", \"question\"]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== BÆ¯á»šC 9: Táº O RAG CHAIN ====================\ndef format_docs(docs):\n    \"\"\"Format documents vá»›i thÃ´ng tin metadata\"\"\"\n    formatted = []\n    for i, doc in enumerate(docs, 1):\n        formatted.append(f\"[TÃ i liá»‡u {i}]\\n{doc.page_content}\")\n    return \"\\n\\n---\\n\\n\".join(formatted)\n\n# RAG chain vá»›i reranker\nrag_chain_with_reranker = (\n    {\n        \"context\": lambda x: format_docs(retrieve_and_rerank(x)),\n        \"question\": RunnablePassthrough()\n    }\n    | PROMPT\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"âœ… Há»‡ thá»‘ng RAG vá»›i Reranker Ä‘Ã£ sáºµn sÃ ng!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== BÆ¯á»šC 10: HÃ€M TIá»†N ÃCH ====================\ndef get_final_verdict(response_text):\n    \"\"\"\n    TrÃ­ch xuáº¥t cÃ¢u tráº£ lá»i ÄÃºng/Sai\n    \"\"\"\n    response_text = response_text.strip().lower()\n    \n    # Æ¯u tiÃªn tá»« Ä‘áº§u tiÃªn\n    first_word = response_text.split()[0] if response_text else \"\"\n    \n    if \"Ä‘Ãºng\" in first_word:\n        return \"TRUE\"\n    elif \"sai\" in first_word:\n        return \"FALSE\"\n    \n    # Fallback\n    if \"Ä‘Ãºng\" in response_text and \"sai\" not in response_text:\n        return \"TRUE\"\n    elif \"sai\" in response_text and \"Ä‘Ãºng\" not in response_text:\n        return \"FALSE\"\n    else:\n        return \"FALSE\"\n\ndef answer_question(question_text, verbose=True):\n    \"\"\"\n    Tráº£ lá»i cÃ¢u há»i vá»›i logging chi tiáº¿t\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"â“ CÃ¢u há»i: {question_text}\")\n    print(f\"{'='*80}\")\n    \n    # Generate answer\n    response_text = rag_chain_with_reranker.invoke(question_text)\n    \n    if verbose:\n        print(f\"\\nğŸ¤– AI Raw Output:\\n{response_text}\")\n    \n    # Parse verdict\n    verdict = get_final_verdict(response_text)\n    print(f\"\\nâœ… Káº¿t luáº­n: {verdict}\")\n    \n    return verdict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== BÆ¯á»šC 11: TEST TRÃŠN Táº¬P KIá»‚M THá»¬ ====================\ndef load_test_data(file_path):\n    \"\"\"\n    Load test data tá»« file Excel\n    \"\"\"\n    import pandas as pd\n    \n    try:\n        df = pd.read_excel(file_path)\n        print(f\"âœ… ÄÃ£ load {len(df)} cÃ¢u há»i test tá»« file\")\n        return df\n    except FileNotFoundError:\n        print(f\"âŒ KhÃ´ng tÃ¬m tháº¥y file: {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"âŒ Lá»—i khi Ä‘á»c file: {e}\")\n        return None\n\ndef evaluate_on_test_set(test_df, verbose=False, save_results=True):\n    \"\"\"\n    ÄÃ¡nh giÃ¡ há»‡ thá»‘ng trÃªn táº­p test vÃ  tÃ­nh accuracy\n    \n    Args:\n        test_df: DataFrame chá»©a test data\n        verbose: In chi tiáº¿t tá»«ng cÃ¢u há»i\n        save_results: LÆ°u káº¿t quáº£ ra file\n    \n    Returns:\n        accuracy: Äá»™ chÃ­nh xÃ¡c (%)\n    \"\"\"\n    if test_df is None:\n        return 0.0\n    \n    results = []\n    correct = 0\n    total = 0\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§ª Báº®T Äáº¦U ÄÃNH GIÃ TRÃŠN Táº¬P TEST\")\n    print(\"=\"*80)\n    print(f\"Tá»•ng sá»‘ cÃ¢u há»i: {len(test_df)}\\n\")\n    \n    for idx, row in test_df.iterrows():\n        question = row['Má»‡nh Ä‘á» CÃ¢u há»i (VIETNAMESE TEXT ONLY)']\n        ground_truth = str(row['ÄÃ¡p Ã¡n (TRUE/FALSE)']).upper()\n        \n        # Kiá»ƒm tra ground truth há»£p lá»‡\n        if ground_truth not in ['TRUE', 'FALSE']:\n            print(f\"âš ï¸  Bá» qua cÃ¢u {idx+1}: Ground truth khÃ´ng há»£p lá»‡ ({ground_truth})\")\n            continue\n        \n        total += 1\n        \n        # Dá»± Ä‘oÃ¡n\n        if verbose:\n            print(f\"\\n{'='*80}\")\n            print(f\"CÃ¢u {idx+1}/{len(test_df)}\")\n            print(f\"{'='*80}\")\n            print(f\"â“ CÃ¢u há»i: {question}\")\n            print(f\"âœ… ÄÃ¡p Ã¡n Ä‘Ãºng: {ground_truth}\")\n        \n        try:\n            # Gá»i há»‡ thá»‘ng RAG\n            response_text = rag_chain_with_reranker.invoke(question)\n            prediction = get_final_verdict(response_text)\n            \n            # Kiá»ƒm tra Ä‘Ãºng/sai\n            is_correct = (prediction == ground_truth)\n            if is_correct:\n                correct += 1\n            \n            if verbose:\n                print(f\"ğŸ¤– Dá»± Ä‘oÃ¡n: {prediction}\")\n                print(f\"{'âœ… ÄÃšNG' if is_correct else 'âŒ SAI'}\")\n                if not is_correct:\n                    print(f\"   Raw output: {response_text[:200]}...\")\n            else:\n                # Hiá»ƒn thá»‹ progress\n                status = \"âœ…\" if is_correct else \"âŒ\"\n                print(f\"{status} CÃ¢u {total}: {prediction} (GT: {ground_truth})\")\n            \n            # LÆ°u káº¿t quáº£\n            results.append({\n                'STT': idx + 1,\n                'Question': question,\n                'Ground_Truth': ground_truth,\n                'Prediction': prediction,\n                'Correct': is_correct,\n                'Raw_Output': response_text\n            })\n            \n        except Exception as e:\n            print(f\"âŒ Lá»—i khi xá»­ lÃ½ cÃ¢u {idx+1}: {e}\")\n            results.append({\n                'STT': idx + 1,\n                'Question': question,\n                'Ground_Truth': ground_truth,\n                'Prediction': 'ERROR',\n                'Correct': False,\n                'Raw_Output': str(e)\n            })\n    \n    # TÃ­nh accuracy\n    accuracy = (correct / total * 100) if total > 0 else 0\n    \n    # In káº¿t quáº£ tá»•ng há»£p\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ\")\n    print(\"=\"*80)\n    print(f\"Tá»•ng sá»‘ cÃ¢u há»i: {total}\")\n    print(f\"Sá»‘ cÃ¢u Ä‘Ãºng: {correct}\")\n    print(f\"Sá»‘ cÃ¢u sai: {total - correct}\")\n    print(f\"Accuracy: {accuracy:.2f}%\")\n    \n    # PhÃ¢n tÃ­ch lá»—i theo loáº¡i\n    if results:\n        import pandas as pd\n        results_df = pd.DataFrame(results)\n        \n        print(\"\\nğŸ“ˆ PhÃ¢n tÃ­ch chi tiáº¿t:\")\n        \n        # Äáº¿m sá»‘ lÆ°á»£ng TRUE/FALSE predictions\n        pred_counts = results_df['Prediction'].value_counts()\n        print(f\"\\nPhÃ¢n bá»‘ dá»± Ä‘oÃ¡n:\")\n        for pred, count in pred_counts.items():\n            print(f\"  - {pred}: {count} cÃ¢u\")\n        \n        # Confusion Matrix\n        print(f\"\\nğŸ¯ Confusion Matrix:\")\n        confusion = pd.crosstab(\n            results_df['Ground_Truth'], \n            results_df['Prediction'],\n            rownames=['Ground Truth'],\n            colnames=['Prediction'],\n            margins=True\n        )\n        print(confusion)\n    \n    # LÆ°u káº¿t quáº£ ra file\n    if save_results and results:\n        import pandas as pd\n        results_df = pd.DataFrame(results)\n        output_file = 'test_results.xlsx'\n        results_df.to_excel(output_file, index=False)\n        print(f\"\\nâœ… ÄÃ£ lÆ°u káº¿t quáº£ chi tiáº¿t vÃ o: {output_file}\")\n        \n        # LÆ°u bÃ¡o cÃ¡o text\n        report_file = 'test_report.txt'\n        with open(report_file, 'w', encoding='utf-8') as f:\n            f.write(\"=\"*80 + \"\\n\")\n            f.write(\"BÃO CÃO ÄÃNH GIÃ Há»† THá»NG RAG\\n\")\n            f.write(\"=\"*80 + \"\\n\\n\")\n            f.write(f\"Tá»•ng sá»‘ cÃ¢u há»i: {total}\\n\")\n            f.write(f\"Sá»‘ cÃ¢u Ä‘Ãºng: {correct}\\n\")\n            f.write(f\"Sá»‘ cÃ¢u sai: {total - correct}\\n\")\n            f.write(f\"Accuracy: {accuracy:.2f}%\\n\\n\")\n            \n            f.write(\"=\"*80 + \"\\n\")\n            f.write(\"CÃC CÃ‚U TRáº¢ Lá»œI SAI\\n\")\n            f.write(\"=\"*80 + \"\\n\\n\")\n            \n            wrong_answers = results_df[results_df['Correct'] == False]\n            for idx, row in wrong_answers.iterrows():\n                f.write(f\"STT: {row['STT']}\\n\")\n                f.write(f\"CÃ¢u há»i: {row['Question']}\\n\")\n                f.write(f\"Ground Truth: {row['Ground_Truth']}\\n\")\n                f.write(f\"Prediction: {row['Prediction']}\\n\")\n                f.write(f\"Raw Output: {row['Raw_Output'][:200]}...\\n\")\n                f.write(\"-\"*80 + \"\\n\\n\")\n        \n        print(f\"âœ… ÄÃ£ lÆ°u bÃ¡o cÃ¡o vÃ o: {report_file}\")\n    \n    return accuracy\n\nif __name__ == \"__main__\":\n    # ÄÆ°á»ng dáº«n file test\n    test_file_path = r\"/kaggle/input/datatest/Test_sample.v1.0.xlsx\"\n    \n    # Load test data\n    test_df = load_test_data(test_file_path)\n    \n    if test_df is not None:\n        # Cháº¡y evaluation\n        accuracy = evaluate_on_test_set(\n            test_df, \n            verbose=False,  # Äáº·t True Ä‘á»ƒ xem chi tiáº¿t tá»«ng cÃ¢u\n            save_results=True\n        )\n        \n        print(f\"\\n{'='*80}\")\n        print(f\"ğŸ¯ ACCURACY CUá»I CÃ™NG: {accuracy:.2f}%\")\n        print(f\"{'='*80}\")\n    else:\n        print(\"\\nâŒ KhÃ´ng thá»ƒ thá»±c hiá»‡n evaluation do khÃ´ng load Ä‘Æ°á»£c test data\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== BÆ¯á»šC 11: TEST ====================\nif __name__ == \"__main__\":\n    # Test vá»›i cÃ¢u há»i máº«u\n    test_questions = [\n        \"ThoÃ¡i hÃ³a khá»›p cÃ¹ng cháº­u cÃ³ thá»ƒ gÃ¢y Ä‘au lÆ°ng, hÃ´ng, cáº£m giÃ¡c tÃª bÃ¬ chÃ¢n.\",\n        \"TÃ¬nh tráº¡ng lá»“i máº¯t á»Ÿ ngÆ°á»i bá»‡nh Basedow thÆ°á»ng sáº½ duy trÃ¬ lÃ¢u dÃ i ngay cáº£ khi ngÆ°á»i bá»‡nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»u trá»‹ bÃ¬nh giÃ¡p.\",\n        \"Trong cÆ¡n sá»‘t rÃ©t, cÃ¡c triá»‡u chá»©ng Ä‘iá»ƒn hÃ¬nh luÃ´n lÃ  rÃ©t run, sá»‘t nÃ³ng vÃ  ra má»“ hÃ´i, khÃ´ng cÃ³ triá»‡u chá»©ng sá»‘t khÃ´ng thÃ nh cÆ¡n.\",\n        \"CÃ¡c thuá»‘c Ä‘iá»u trá»‹ bÃ©o phÃ¬ thÆ°á»ng khÃ´ng mang láº¡i hiá»‡u quáº£ cao náº¿u khÃ´ng káº¿t há»£p vá»›i cháº¿ Ä‘á»™ Äƒn giáº£m cÃ¢n vÃ  tÄƒng cÆ°á»ng váº­n Ä‘á»™ng thá»ƒ lá»±c.\",\n        \"TÄƒng huyáº¿t Ã¡p háº­u sáº£n chá»‰ lÃ  vÃ´ cÄƒn (khÃ´ng rÃµ nguyÃªn nhÃ¢n) vÃ  khÃ´ng gÃ¢y biáº¿n chá»©ng nguy hiá»ƒm.\",\n        \"ViÃªm tháº­n bá»ƒ tháº­n khÃ´ng Ä‘Æ°á»£c coi lÃ  NTTNDT phá»©c táº¡p.\"\n    ]\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§ª Báº®T Äáº¦U TEST Há»† THá»NG\")\n    print(\"=\"*80)\n    \n    for i, question in enumerate(test_questions, 1):\n        print(f\"\\n--- Test {i}/{len(test_questions)} ---\")\n        answer_question(question, verbose=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}