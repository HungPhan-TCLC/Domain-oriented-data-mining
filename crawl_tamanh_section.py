import json
import re
import time
import hashlib
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException


# =========================
# CONFIG
# =========================
AZ_URL = "https://tamanhhospital.vn/benh-hoc-a-z/"
OUT_JSONL = "tamanh_sections.jsonl"

HEADLESS = True
WAIT_SEC = 25
SCROLL_ROUNDS = 8
SLEEP_BETWEEN_PAGES_SEC = 0.3  # giáº£m táº£i server + á»•n Ä‘á»‹nh

# test nhanh: Ä‘áº·t LIMIT_DISEASES = 5; khi cháº¡y tháº­t set None
LIMIT_DISEASES = None

# Bá» qua section quÃ¡ ngáº¯n (Ä‘á»¡ nhiá»…u)
MIN_CONTENT_CHARS = 120


# =========================
# Driver
# =========================
def create_driver(headless: bool = True) -> webdriver.Chrome:
    opts = webdriver.ChromeOptions()
    if headless:
        opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1400,900")
    driver = webdriver.Chrome(options=opts)
    return driver


# =========================
# Helpers
# =========================

def is_faq_section(title: str) -> bool:
    t = title.lower()
    return any(k in t for k in [
        "cÃ¢u há»i thÆ°á»ng gáº·p",
        "cÃ¢u há»i hay gáº·p",
        "tháº¯c máº¯c",
        "giáº£i Ä‘Ã¡p tháº¯c máº¯c",
        "há»i Ä‘Ã¡p"
    ])

def clean_text(s: str) -> str:
    s = s.strip()
    s = re.sub(r"\r\n", "\n", s)
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()


def slug_from_url(url: str) -> str:
    path = urlparse(url).path.rstrip("/")
    slug = path.split("/")[-1] if path else ""
    return slug or "unknown"


def stable_id(url: str, anchor: str) -> str:
    # ID á»•n Ä‘á»‹nh theo (url + anchor), khÃ´ng phá»¥ thuá»™c index
    h = hashlib.md5(f"{url}#{anchor}".encode("utf-8")).hexdigest()[:10]
    return f"TA_{h}"


def map_category(title: str) -> str:
    t = title.lower()

    # Triá»‡u chá»©ng / dáº¥u hiá»‡u
    if any(k in t for k in ["triá»‡u chá»©ng", "dáº¥u hiá»‡u", "biáº¿n chá»©ng", "biá»ƒu hiá»‡n"]):
        return "Triá»‡u chá»©ng/Dáº¥u hiá»‡u"

    # Thuá»‘c / Ä‘iá»u trá»‹ / phÃ²ng ngá»«a
    if any(
        k in t
        for k in [
            "Ä‘iá»u trá»‹",
            "thuá»‘c",
            "phÃ²ng ngá»«a",
            "biá»‡n phÃ¡p",
            "chÄƒm sÃ³c",
            "cÃ¡ch phÃ²ng",
            "phÃ²ng bá»‡nh",
        ]
    ):
        return "Thuá»‘c"

    # CÃ²n láº¡i: Bá»‡nh (bao gá»“m lÃ  gÃ¬, phÃ¢n loáº¡i, phá»• biáº¿n, nguyÃªn nhÃ¢n, cháº©n Ä‘oÃ¡n...)
    return "Bá»‡nh"


def scroll_to_bottom(driver: webdriver.Chrome, rounds: int = 6, sleep_sec: float = 1.0):
    last_h = driver.execute_script("return document.body.scrollHeight")
    for i in range(rounds):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(sleep_sec)
        new_h = driver.execute_script("return document.body.scrollHeight")
        if new_h == last_h:
            break
        last_h = new_h


# =========================
# Aâ€“Z: get disease URLs
# =========================
def get_disease_urls_from_az(driver: webdriver.Chrome) -> List[str]:
    print("[INFO] Open Aâ€“Z page:", AZ_URL)
    driver.get(AZ_URL)

    WebDriverWait(driver, WAIT_SEC).until(
        EC.presence_of_element_located((By.TAG_NAME, "body"))
    )
    scroll_to_bottom(driver, rounds=SCROLL_ROUNDS, sleep_sec=1.0)

    soup = BeautifulSoup(driver.page_source, "lxml")

    urls = []
    seen = set()

    for a in soup.select("a[href*='/benh/']"):
        href = (a.get("href") or "").strip()
        if not href:
            continue
        if href.startswith("/"):
            href = urljoin("https://tamanhhospital.vn", href)

        # chá»‰ láº¥y dáº¡ng .../benh/<slug>/
        if not re.search(r"^https?://(www\.)?tamanhhospital\.vn/benh/[^/]+/?$", href):
            continue

        if href in seen:
            continue
        seen.add(href)
        urls.append(href)

    print(f"[INFO] Found {len(urls)} disease URLs from Aâ€“Z")
    return urls


# =========================
# TOC extraction (robust)
# =========================
def find_toc_container(soup: BeautifulSoup):
    """
    TÃ¬m block chá»©a chá»¯ 'Má»¥c lá»¥c' vÃ  cÃ¡c link #anchor.
    Heuristic: tÃ¬m text 'Má»¥c lá»¥c', rá»“i leo lÃªn vÃ i cáº¥p Ä‘áº¿n khi tháº¥y <a href="#...">
    """
    label = soup.find(string=lambda x: isinstance(x, str) and x.strip().lower() == "má»¥c lá»¥c")
    if not label:
        return None

    node = label.parent
    for _ in range(6):
        if not node:
            break
        if node.find_all("a", href=re.compile(r"^#")):
            return node
        node = node.parent
    return None


def extract_toc_items_h2(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """
    Tráº£ vá» list cÃ¡c má»¥c lá»¥c cáº¥p 1 (Æ°u tiÃªn h2):
      [{title, anchor}]
    Náº¿u TOC cÃ³ má»¥c con (1.1/1.2) -> thÆ°á»ng tÆ°Æ¡ng á»©ng h3; ta lá»c Ä‘á»ƒ giá»¯ má»¥c cáº¥p 1.
    """
    toc = find_toc_container(soup)
    if not toc:
        return []

    # láº¥y táº¥t cáº£ link #...
    raw = []
    for a in toc.find_all("a", href=re.compile(r"^#")):
        title = a.get_text(" ", strip=True)
        href = (a.get("href") or "").strip()
        if not title or not href.startswith("#"):
            continue
        raw.append({"title": title, "anchor": href[1:]})

    # dedup
    seen = set()
    dedup = []
    for it in raw:
        key = (it["title"], it["anchor"])
        if key in seen:
            continue
        seen.add(key)
        dedup.append(it)

    # Lá»c: chá»‰ giá»¯ nhá»¯ng anchor trá» tá»›i H2 (má»¥c cáº¥p 1)
    items = []
    for it in dedup:
        target = soup.find(id=it["anchor"])
        if not target:
            continue
        if target.name and target.name.lower() == "h2":
            items.append(it)

    # Náº¿u lá»c xong mÃ  rá»—ng, fallback: giá»¯ raw (váº«n cáº¯t theo heading id, dá»«ng á»Ÿ h2)
    return items if items else dedup


# =========================
# Section extraction: from H2 to next H2
# =========================
CONTENT_TAGS = {"p", "li", "ul", "ol", "table", "blockquote"}


def extract_section_content_from_h2(soup: BeautifulSoup, anchor_id: str) -> str:
    """
    - Start: heading id=anchor_id (thÆ°á»ng lÃ  h2)
    - Collect: p, li, ul/ol/table/blockquote dÆ°á»›i section
    - Stop: gáº·p h2 tiáº¿p theo (má»¥c cáº¥p 1 má»›i)
    """
    start = soup.find(id=anchor_id)
    if not start:
        return ""

    parts: List[str] = []

    # Duyá»‡t cÃ¡c node sau start theo thá»© tá»± tÃ i liá»‡u
    for node in start.find_all_next():
        # stop á»Ÿ h2 má»›i
        if node is not start and node.name and node.name.lower() == "h2":
            break

        # láº¥y ná»™i dung cÃ¡c tag giÃ u text
        if node.name in CONTENT_TAGS:
            txt = node.get_text(" ", strip=True)
            if txt:
                parts.append(txt)

    return clean_text("\n".join(parts))


# =========================
# Crawl one disease page -> many records
# =========================
def crawl_one_disease_sections(driver: webdriver.Chrome, url: str) -> Tuple[str, List[Dict]]:
    driver.get(url)
    WebDriverWait(driver, WAIT_SEC).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

    soup = BeautifulSoup(driver.page_source, "lxml")

    h1 = soup.find("h1")
    disease_title = h1.get_text(" ", strip=True) if h1 else slug_from_url(url)

    toc_items = extract_toc_items_h2(soup)
    if not toc_items:
        print(f"[WARN] No TOC found: {url}")
        return disease_title, []

    records = []
    for it in toc_items:
        title = it["title"].strip()
        anchor = it["anchor"].strip()

        # ðŸš« Bá»Ž FAQ / THáº®C Máº®C
        if is_faq_section(title):
            continue

        content = extract_section_content_from_h2(soup, anchor)
        if len(content) < MIN_CONTENT_CHARS:
            continue

        rec = {
            "id": stable_id(url, title),
            "category": map_category(title),
            "title": title,
            "content": content,
            "source": "tamanhhospital",
            "url": url
        }
        records.append(rec)


    return disease_title, records


# =========================
# Main pipeline
# =========================
def main(limit_diseases: Optional[int] = None):
    driver = create_driver(headless=HEADLESS)
    total_sections = 0
    total_diseases = 0

    try:
        urls = get_disease_urls_from_az(driver)
        if limit_diseases:
            urls = urls[:limit_diseases]
            print(f"[INFO] LIMIT diseases = {limit_diseases}")

        with open(OUT_JSONL, "w", encoding="utf-8") as f:
            for i, url in enumerate(urls, start=1):
                print(f"\n[INFO] [{i}/{len(urls)}] Crawl disease: {url}")
                try:
                    disease_title, records = crawl_one_disease_sections(driver, url)
                except TimeoutException:
                    print("[ERROR] Timeout:", url)
                    continue
                except WebDriverException as e:
                    print("[ERROR] WebDriverException:", e)
                    continue
                except Exception as e:
                    print("[ERROR] Unknown error:", e)
                    continue

                total_diseases += 1
                print(f"[INFO] Disease title: {disease_title}")
                print(f"[INFO] Sections extracted: {len(records)}")

                for r in records:
                    f.write(json.dumps(r, ensure_ascii=False) + "\n")
                total_sections += len(records)

                time.sleep(SLEEP_BETWEEN_PAGES_SEC)

        print("\n[DONE]")
        print("  Diseases processed:", total_diseases)
        print("  Sections saved    :", total_sections)
        print("  Output file       :", OUT_JSONL)

    finally:
        driver.quit()


if __name__ == "__main__":
    main(limit_diseases=LIMIT_DISEASES)
